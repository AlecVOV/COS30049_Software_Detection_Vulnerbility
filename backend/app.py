import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ml_detector import VulnerabilityDetector
from vuln_rules import RuleBasedDetector
import os

# Page config
st.set_page_config(
    page_title="SecureCode AI - Vulnerability Detector",
    page_icon="üîí",
    layout="wide"
)

# Initialize detectors
@st.cache_resource
def load_models():
    try:
        ml_detector = VulnerabilityDetector()
        rule_detector = RuleBasedDetector()
        return ml_detector, rule_detector, None
    except Exception as e:
        return None, None, str(e)

ml_detector, rule_detector, error = load_models()

if error:
    st.error(f"‚ö†Ô∏è Error loading models: {error}")
    st.info("Please run `python train_model.py` first to train the models.")
    st.stop()

# Sidebar navigation
st.sidebar.title("üîí SecureCode AI")
page = st.sidebar.radio("Navigation", 
    ["üè† Home", "üìä Data Analysis", "üîç Scan Code", "üìà Model Comparison"])

# ====================
# HOME PAGE
# ====================
if page == "üè† Home":
    st.title("SecureCode AI - Vulnerability Detection System")
    
    st.markdown("""
    ### üéØ Project Overview
    An AI-powered system to detect security vulnerabilities in source code using:
    - **Machine Learning Models** (Logistic Regression + Random Forest + SVM)
    - **Rule-Based Detection** (Pattern matching for known vulnerabilities)
    
    ### ü§ñ Machine Learning Models
    1. **Logistic Regression** - Fast, interpretable baseline
    2. **Random Forest** - Handles complex patterns
    3. **Support Vector Machine (SVM)** - High accuracy with kernel tricks
    
    ### üìä Dataset
    - **DiverseVul**: 18,945 vulnerable code samples
    - Multiple programming languages supported
    
    ### üîß Supported Languages
    Python, JavaScript, Java, C, C++
    """)
    
    # Show quick stats
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Samples", "330,000")
    with col2:
        st.metric("ML Models", "6")
    with col3:
        st.metric("Best Accuracy", "70.40%")
    with col4:
        st.metric("Detection Methods", "4")

# ====================
# DATA ANALYSIS PAGE
# ====================
elif page == "üìä Data Analysis":
    st.title("üìä Data Analysis & Visualization")
    
    # Load dataset
    @st.cache_data
    def load_data():
        return pd.read_csv('data/merged_all_datasets.csv')
    
    try:
        df = load_data()
        
        st.subheader("Dataset Overview")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Samples", f"{len(df):,}")
        with col2:
            st.metric("Vulnerable", f"{sum(df['is_vulnerable'] == 1):,}")
        with col3:
            st.metric("Safe", f"{sum(df['is_vulnerable'] == 0):,}")
        
        st.dataframe(df.head(10), width='stretch')
        
        # Class distribution
        st.subheader("Class Distribution")
        col1, col2 = st.columns(2)
        
        with col1:
            fig, ax = plt.subplots(figsize=(8, 6))
            df['is_vulnerable'].value_counts().plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'])
            ax.set_title('Vulnerable vs Safe Code', fontsize=14, fontweight='bold')
            ax.set_xlabel('is_vulnerable')
            ax.set_ylabel('Count')
            ax.set_xticklabels(['Safe', 'Vulnerable'], rotation=0)
            st.pyplot(fig)
        
        with col2:
            # Code length distribution
            df['code_length'] = df['code'].str.len()
            fig, ax = plt.subplots(figsize=(8, 6))
            df[df['is_vulnerable'] == 0]['code_length'].hist(alpha=0.6, bins=50, ax=ax, 
                label='Safe', color='#2ecc71')
            df[df['is_vulnerable'] == 1]['code_length'].hist(alpha=0.6, bins=50, ax=ax, 
                label='Vulnerable', color='#e74c3c')
            ax.set_title('Code Length Distribution', fontsize=14, fontweight='bold')
            ax.set_xlabel('Code Length (characters)')
            ax.set_ylabel('Frequency')
            ax.legend()
            st.pyplot(fig)
            
    except FileNotFoundError:
        st.error("Dataset not found. Please ensure data/chunked_vuln_dataset.csv exists.")

# ====================
# SCAN CODE PAGE
# ====================
elif page == "üîç Scan Code":
    st.title("üîç Vulnerability Scanner")
    
    # Input method selection
    input_method = st.radio("Choose input method:", 
        ["üìù Paste Code", "üìÅ Upload File"])
    
    code_input = ""
    
    if input_method == "üìù Paste Code":
        code_input = st.text_area("Paste your code here:", height=300,
            placeholder="# Paste your code here...\ndef example():\n    pass")
        language = st.selectbox("Select Language:", 
            ["Python", "JavaScript", "Java", "C", "C++"])
    
    else:
        uploaded_file = st.file_uploader("Upload source code file", 
            type=['py', 'js', 'java', 'c', 'cpp', 'txt'])
        if uploaded_file:
            code_input = uploaded_file.read().decode('utf-8')
            st.code(code_input, language='python')
    
    if st.button("üîç Scan for Vulnerabilities", type="primary", width='stretch'):
        if code_input:
            with st.spinner("Analyzing code with 6 ML models + rule-based detection..."):
                # ML Detection
                ml_result = ml_detector.predict(code_input)
                
                # Rule-based Detection
                rule_result = rule_detector.analyze(code_input, language.lower())
                
                # Display results
                st.markdown("---")
                st.subheader("üéØ Scan Results")
                
                # Overall result
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if ml_result['is_vulnerable']:
                        st.error("üî¥ VULNERABLE")
                    else:
                        st.success("üü¢ SAFE")
                    st.metric("Ensemble Confidence", f"{ml_result['confidence']:.1%}")
                
                with col2:
                    st.metric("Risk Level", ml_result['risk_level'])
                    st.metric("Ensemble Probability", f"{ml_result['ensemble_probability']:.1%}")
                
                with col3:
                    st.metric("Rule-Based Issues", len(rule_result))
                    st.write("**Voting:**")
                    st.write(f"Vulnerable: {ml_result['voting']['vulnerable']}")
                    st.write(f"Safe: {ml_result['voting']['safe']}")
                
                # Individual model predictions
                st.markdown("---")
                st.subheader("ü§ñ Individual Model Predictions")
                
                model_data = []
                for model_name, pred in ml_result['model_predictions'].items():
                    model_data.append({
                        'Model': model_name,
                        'Prediction': 'üî¥ Vulnerable' if pred['vulnerable'] else 'üü¢ Safe',
                        'Confidence': f"{pred['confidence']:.1%}",
                        'Safe Probability': f"{pred['safe_prob']:.1%}",
                        'Vulnerable Probability': f"{pred['vuln_prob']:.1%}"
                    })
                
                df_models = pd.DataFrame(model_data)
                st.dataframe(df_models, width='stretch', hide_index=True)
                
                # Visualization of probabilities
                fig, ax = plt.subplots(figsize=(10, 5))
                models = list(ml_result['model_predictions'].keys())
                vuln_probs = [ml_result['model_predictions'][m]['vuln_prob'] for m in models]
                safe_probs = [ml_result['model_predictions'][m]['safe_prob'] for m in models]
                
                x = range(len(models))
                width = 0.35
                
                ax.bar([i - width/2 for i in x], safe_probs, width, label='Safe', color='#2ecc71', alpha=0.8)
                ax.bar([i + width/2 for i in x], vuln_probs, width, label='Vulnerable', color='#e74c3c', alpha=0.8)
                
                ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Threshold')
                ax.set_ylabel('Probability')
                ax.set_title('Model Predictions Comparison', fontweight='bold')
                ax.set_xticks(x)
                ax.set_xticklabels(models, rotation=15, ha='right')
                ax.legend()
                ax.grid(axis='y', alpha=0.3)
                
                st.pyplot(fig)
                
                # Rule-based results
                if rule_result:
                    st.markdown("---")
                    st.subheader("‚ö†Ô∏è Rule-Based Detection Results")
                    st.error(f"Found {len(rule_result)} potential security issues:")
                    
                    for i, issue in enumerate(rule_result, 1):
                        with st.expander(f"#{i} - {issue['type']} ({issue['severity']} severity)", expanded=True):
                            st.write(f"**Description:** {issue['description']}")
                            st.code(issue['code_snippet'], language='python')
                else:
                    st.markdown("---")
                    st.success("‚úÖ No rule-based vulnerabilities detected")
                    
        else:
            st.warning("‚ö†Ô∏è Please provide code to scan!")

# ====================
# MODEL COMPARISON PAGE
# ====================
elif page == "üìà Model Comparison":
    st.title("üìà Model Performance Comparison")
    
    st.markdown("""
    ### ü§ñ Models Evaluated
    1. **Logistic Regression** - Linear classifier with L2 regularization
    2. **Random Forest** - Ensemble of 100 decision trees
    3. **Gradient Boosting** - Sequential boosting with decision trees
    4. **XGBoost** - Gradient boosting framework
    5. **LightGBM** - Fast gradient boosting (LightGBM)
    6. **Naive Bayes** - Probabilistic classifier with Laplace smoothing
    """)
    
    # Load comparison data if exists
    comparison_file = 'docs/model_comparison.csv'
    
    if os.path.exists(comparison_file):
        df_metrics = pd.read_csv(comparison_file)
        
        st.subheader("üìä Performance Metrics")
        st.dataframe(df_metrics, width='stretch', hide_index=True)
        
        # Highlight best model
        best_model = df_metrics.loc[df_metrics['Accuracy'].idxmax(), 'Model']
        best_acc = df_metrics['Accuracy'].max()
        st.success(f"üèÜ **Best Model:** {best_model} with {best_acc:.2%} accuracy")
        
        # Visualizations
        if os.path.exists('docs/accuracy_vs_speed.png'):
            st.image('docs/accuracy_vs_speed.png', 
                caption='Accuracy vs Speed Comparison')

        # Confusion matrices
        st.subheader("üîç Confusion Matrices")
        
        cols = st.columns(6)
        for idx, model_name in enumerate(df_metrics['Model']):
            filename = model_name.lower().replace(' ', '_')
            img_path = f'docs/confusion_matrix_{filename}.png'
            
            if os.path.exists(img_path):
                with cols[idx]:
                    st.image(img_path, caption=model_name, width='stretch')
    
        # Add after Chart 4 (around line 290)

        st.markdown("---")
        
        
        
        # Charts 4 & 5: Side by side
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ROC Curves")
            if os.path.exists('docs/roc_curves.png'):
                st.image('docs/roc_curves.png', 
                        use_container_width=True,
                        caption='Receiver Operating Characteristic curves showing true positive vs false positive rates')
                
        with col2:
            st.markdown("### Precision-Recall Curves")
            if os.path.exists('docs/precision_recall_curves.png'):
                st.image('docs/precision_recall_curves.png', 
                        use_container_width=True,
                        caption='Trade-off between precision and recall at various thresholds')
                
    else:
        st.warning("‚ö†Ô∏è Model comparison data not found. Please run `python train_model.py` first.")
    
    # Model justification
    st.markdown("---")
    st.subheader("üí° Model Selection Justification")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        **Logistic Regression**
        - ‚úÖ Fast training & prediction
        - ‚úÖ Good baseline performance
        - ‚úÖ Interpretable coefficients
        - ‚úÖ Low computational cost
        - ‚ö†Ô∏è Assumes linear separability
        """)
    
    with col2:
        st.markdown("""
        **Random Forest**
        - ‚úÖ Handles non-linear patterns
        - ‚úÖ Robust to overfitting
        - ‚úÖ Feature importance analysis
        - ‚úÖ No feature scaling needed
        - ‚ö†Ô∏è Slower prediction time
        """)
    
    with col3:
        st.markdown("""
        **Gradient Boosting**
        - ‚úÖ High accuracy
        - ‚úÖ Handles complex relationships
        - ‚úÖ Customizable loss functions
        - ‚úÖ Works well with imbalanced data
        - ‚ö†Ô∏è Longer training time
        """)
    
    
    col4, col5, col6 = st.columns(3)
    with col4:
        st.markdown("""
        **XGBoost**
        - ‚úÖ State-of-the-art boosting
        - ‚úÖ Handles missing data
        - ‚úÖ Regularization to reduce overfitting
        - ‚úÖ Parallel processing support
        - ‚ö†Ô∏è More hyperparameters to tune
        """)
        
    with col5:
        st.markdown("""
        **LightGBM**
        - ‚úÖ Faster training than XGBoost
        - ‚úÖ Lower memory usage
        - ‚úÖ Good for large datasets
        - ‚úÖ Supports categorical features
        - ‚ö†Ô∏è Sensitive to overfitting
        """)
        
    with col6:
        st.markdown("""
        **Naive Bayes**
        - ‚úÖ Simple & fast
        - ‚úÖ Works well with small data
        - ‚úÖ Handles high-dimensional data
        - ‚úÖ Probabilistic output
        - ‚ö†Ô∏è Assumes feature independence
        """)
    
    st.markdown("---")
    st.info("""
    **Ensemble Strategy:** We combine predictions from all three models using probability averaging,
    which improves overall accuracy and reduces the risk of individual model errors.
    """)

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**SecureCode AI v2.0**")
st.sidebar.markdown("COS30049: Computing Technology Innovation Project - Assignment 2")
st.sidebar.markdown("6 ML Models + Rules-based system")